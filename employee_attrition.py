# -*- coding: utf-8 -*-
"""Employee-Attrition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1woWbVBPPEslV8nFahQNUVafs0_OjNJTL

### Predecting employee Attrition
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline

from google.colab import files
files.upload()

df=pd.read_csv("WA_Fn-UseC_-HR-Employee-Attrition.csv")

df.head()

df.info()

df.columns

df.describe()

df.isnull().sum()

df.isnull().any()

fig=plt.figure(figsize=(20,15))
sns.pairplot(df)

df.corr()

df.shape

fig=plt.figure(figsize=(5,5))
sns.countplot(df['Attrition'],palette='colorblind')

sns.distplot(df["MonthlyIncome"],kde=False)
mean_inc = df["MonthlyIncome"].mean()
plt.axvline(mean_inc,c="r",label="Mean Income")
plt.legend()
plt.show()

sns.catplot(x="JobLevel",y="MonthlyIncome",data=df)
plt.title("Job level on monthly income")
plt.show()

fig=plt.figure(figsize=(5,5))
sns.catplot(x="Attrition",kind="count",col="Gender",data=df);



for column in df.columns:
  if df[column].dtype==object:
    print(str(column)+ ' : '+ str(df[column].unique()))
    print(df[column].value_counts())
    print('_______________________________________________')

df=df.drop(['Over18','EmployeeNumber'],axis=1)
df=df.drop(['StandardHours','EmployeeCount'],axis=1)

df.info()

fig=plt.figure(figsize=(20,15))
sns.heatmap(df.corr(),cmap='magma',annot=True)

from sklearn.preprocessing import LabelEncoder

for column in df.columns:
  if df[column].dtype == np.number:
    continue
  df[column]=LabelEncoder().fit_transform(df[column])

df['Age_Years']=df['Age']

df=df.drop('Age',axis=1)

df.head()

X=df.drop('Attrition',axis=1)
Y=df['Attrition']

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.25,random_state=42)

from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

"""### Applying Logisitc Regression"""

from sklearn.linear_model import LogisticRegression
LR= LogisticRegression()
LR.fit(x_train,y_train)

print(LR.predict(x_train))

y_pred1=LR.predict(x_test)

print(y_pred1)

from sklearn.metrics import classification_report,accuracy_score,confusion_matrix

print(classification_report(y_test,y_pred1))

fig=plt.figure(figsize=(5,5))
cf_matrix=confusion_matrix(y_test,y_pred1)
print(cf_matrix)

sns.heatmap(cf_matrix,annot=True)

print(accuracy_score(y_test,y_pred1))

"""### Applying K-Nearest Neighbors"""

KNN=KNeighborsClassifier(n_neighbors=10)

KNN.fit(x_train,y_train)

y_pred5=KNN.predict(x_test)

cf_matrix=classification_report(y_test,y_pred5)

print(accuracy_score(y_pred5,y_test))

cf_matrix=confusion_matrix(y_test,y_pred5)
print(cf_matrix)
sns.heatmap(cf_matrix,annot=True)

"""### Applying Decision Trees"""

dt=DecisionTreeClassifier()

dt.fit(x_train,y_train)

y_pred=dt.predict(x_test)
print(y_pred)

print(classification_report(y_test,y_pred))

print(confusion_matrix(y_test,y_pred))

print(accuracy_score(y_test,y_pred))

cf_matrix=confusion_matrix(y_test,y_pred)
print(cf_matrix)
sns.heatmap(cf_matrix,annot=True)

!pip install graphviz

!pip install pydotplus

from pydotplus.graphviz import graph_from_dot_data
from sklearn.tree import export_graphviz
from sklearn.tree import plot_tree

#dot_data=export_graphviz(dt,out_file=None,max_depth = 5 , filled=True , rounded=True,feature_names=x_train.columns)

#graph = graph_from_dot_data(dot_data)                 # Create graph from dot data
#graph.write_png('dt.png')

x_train.columns

fig=plt.figure(figsize=(25,15))
A=plot_tree(dt,max_depth = 3 , filled=True , rounded=True,feature_names=x_train.columns,fontsize=25)

"""### Applying Random Forest"""

from sklearn.ensemble import RandomForestClassifier
RF = RandomForestClassifier(n_estimators=10,criterion = 'entropy')
RF.fit(x_train,y_train)

y_pred2=RF.predict(x_test)

cm= confusion_matrix(y_test,y_pred2)
print(cm)

print(accuracy_score(y_test,y_pred2))

cf_matrix=confusion_matrix(y_test,y_pred2)
print(cf_matrix)
sns.heatmap(cf_matrix,annot=True)

"""### Applying SVC"""

sv=SVC()
sv.fit(x_train,y_train)

y_pred6=sv.predict(x_test)

print(classification_report(y_test,y_pred6))

print(confusion_matrix(y_test,y_pred6))

cf_matrix=confusion_matrix(y_test,y_pred6)
print(cf_matrix)
sns.heatmap(cf_matrix,annot=True)

print(accuracy_score(y_test,y_pred2))

"""### Applying Ensemble Techniques like XGBoost"""

import xgboost as xgb

XGB= xgb.XGBClassifier()
XGB.fit(x_train,y_train)

y_pred4=XGB.predict(x_test)

print(classification_report(y_test,y_pred4))

cf_matrix=confusion_matrix(y_test,y_pred4)
print(cf_matrix)
sns.heatmap(cf_matrix,annot=True)

print(accuracy_score(y_test,y_pred4))



"""### Applying Cross-Validation on the Boosted model"""

from sklearn.model_selection import cross_val_score

scores = cross_val_score(XGB, X,Y,scoring="accuracy", cv=10)

print(scores)

scores.mean()





